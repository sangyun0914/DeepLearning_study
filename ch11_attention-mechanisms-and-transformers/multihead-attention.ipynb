{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3f9f9b2",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# Multi-Head Attention\n",
    ":label:`sec_multihead-attention`\n",
    "\n",
    "## Section Summary\n",
    "This section discusses the concept of multi-head attention, which is an extension of the attention mechanism. In traditional attention, the queries, keys, and values are transformed using a single linear projection and then fed into attention pooling. However, in multi-head attention, the queries, keys, and values are transformed independently using multiple linear projections, resulting in multiple attention pooling outputs called \"heads.\" These heads are then concatenated and transformed again to produce the final output.\n",
    "\n",
    "The advantage of multi-head attention is that it allows the model to capture dependencies of various ranges within a sequence by combining knowledge from different behaviors of the attention mechanism. Each head can attend to different parts of the input, enabling more expressive and sophisticated computations.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b5cd93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T05:16:47.496420Z",
     "iopub.status.busy": "2023-02-10T05:16:47.496051Z",
     "iopub.status.idle": "2023-02-10T05:16:50.304549Z",
     "shell.execute_reply": "2023-02-10T05:16:50.302874Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27c35e94",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "\n",
    "## Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1a0fb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T05:16:50.309828Z",
     "iopub.status.busy": "2023-02-10T05:16:50.309187Z",
     "iopub.status.idle": "2023-02-10T05:16:50.427682Z",
     "shell.execute_reply": "2023-02-10T05:16:50.426985Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(d2l.Module):  #@save\n",
    "    \"\"\"Multi-head attention.\"\"\"\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries = self.transpose_qkv(self.W_q(queries))\n",
    "        keys = self.transpose_qkv(self.W_k(keys))\n",
    "        values = self.transpose_qkv(self.W_v(values))\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        output_concat = self.transpose_output(output)\n",
    "        return self.W_o(output_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "251e7fda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T05:16:50.430745Z",
     "iopub.status.busy": "2023-02-10T05:16:50.430430Z",
     "iopub.status.idle": "2023-02-10T05:16:50.436302Z",
     "shell.execute_reply": "2023-02-10T05:16:50.435636Z"
    },
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MultiHeadAttention)  #@save\n",
    "def transpose_qkv(self, X):\n",
    "    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
    "    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "@d2l.add_to_class(MultiHeadAttention)  #@save\n",
    "def transpose_output(self, X):\n",
    "    \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
    "    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09588345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T05:16:50.439286Z",
     "iopub.status.busy": "2023-02-10T05:16:50.438982Z",
     "iopub.status.idle": "2023-02-10T05:16:50.505281Z",
     "shell.execute_reply": "2023-02-10T05:16:50.504267Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sangyun/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n",
    "batch_size, num_queries, num_kvpairs = 2, 4, 6\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "d2l.check_shape(attention(X, Y, Y, valid_lens),\n",
    "                (batch_size, num_queries, num_hiddens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
