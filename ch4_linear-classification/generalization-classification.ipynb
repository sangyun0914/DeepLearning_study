{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13f0a4ba",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Generalization in Classification\n",
    "\n",
    ":label:`chap_classification_generalization`\n",
    "\n",
    "## Section Summary\n",
    "This section discusses the challenge of generalization in machine learning and how it relates to deep neural networks. While linear models can overfit by memorizing the training set, deep neural networks can generalize well with far fewer examples. The chapter introduces some foundational ideas of statistical learning theory, which provide guarantees for generalization, but these guarantees are of limited practical use for deep learning practitioners. Instead, practitioners often rely on methods that have generalized well on similar problems in the past and certify generalization through empirical evaluations. The chapter promises to revisit generalization in the context of perceptrons and provide an introduction to the scientific literature that has attempted to explain why deep neural networks generalize well in practice.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The Test Set\n",
    "The text discusses the concept of test sets and their properties in assessing generalization error. It distinguishes between empirical error, which is the fraction of instances that disagree with the true label, and population error, which is the expected fraction of instances in the underlying population that disagree with the true label. The central limit theorem suggests that as the number of examples grows large, the test error should approach the true error at a rate of O(1/sqrt(n)). The article concludes that if one wants test error to approximate the population error such that one standard deviation corresponds to an interval of Â±0.01, one should collect roughly 2500 samples.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Test Set Reuse\n",
    "The text discusses the importance of test sets in machine learning research, particularly in evaluating the performance of classifiers. It emphasizes the need to set aside an appropriate number of examples for the test set, and to conduct all preliminary analysis and model selection on a separate validation set. However, the text also highlights the problems that arise when evaluating multiple classifiers on the same test set, particularly the issue of false discovery due to multiple hypothesis testing, and the problem of adaptive overfitting when subsequent models are chosen based on the performance of previous models. To mitigate these problems, the text suggests creating multiple test sets and using them infrequently, accounting for multiple hypothesis testing, and being vigilant when the stakes are high and dataset size is small.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Statistical Learning Theory\n",
    "\n",
    "The text discusses the limitations of test sets in machine learning and introduces the field of statistical learning theory, which aims to understand why and when models trained on empirical data can generalize to unseen data. Learning theorists aim to bound the difference between the empirical error of a learned classifier on a training set and the true error of that same classifier on the underlying population. One of the key contributions of this field is the Vapnik-Chervonenkis (VC) dimension, which measures the complexity of a model class, and their work provides a bound for the difference between the empirical error and the population error as a function of the VC dimension and the number of samples. The central question of learning theory is to develop the appropriate mathematical analysis to quantify where a model sits along the spectrum between more flexible and more rigid model classes, and to provide associated guarantees.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. If we wish to estimate the error of a fixed model $f$\n",
    "   to within $0.0001$ with probability greater than 99.9%,\n",
    "   how many samples do we need?\n",
    "   - 10^7 samples\n",
    "1. Suppose that somebody else possesses a labeled test set\n",
    "   $\\mathcal{D}$ and only makes available the unlabeled inputs (features).\n",
    "   Now suppose that you can only access the test set labels\n",
    "   by running a model $f$ (no restrictions placed on the model class)\n",
    "   on each of the unlabeled inputs\n",
    "   and receiving the corresponding error $\\epsilon_\\mathcal{D}(f)$.\n",
    "   How many models would you need to evaluate\n",
    "   before you leak the entire test set\n",
    "   and thus could appear to have error $0$,\n",
    "   regardless of your true error?\n",
    "1. What is the VC dimension of the class of $5^\\mathrm{th}$-order polynomials?\n",
    "   - At most 6 dimension\n",
    "1. What is the VC dimension of axis-aligned rectangles on two-dimensional data?\n",
    "   - 4 dimension\n",
    "\n",
    "[Discussions](https://discuss.d2l.ai/t/6829)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
