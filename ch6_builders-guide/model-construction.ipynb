{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb9ad85f",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# Layers and Modules\n",
    ":label:`sec_model_construction`\n",
    "\n",
    "## Section Summary\n",
    "The section talks about layers, modules, and neural network models. Layers are groups of neurons that take inputs and generate corresponding outputs, and layers are described by tunable parameters. A module can describe a single layer or a component consisting of multiple layers. Modules can be combined into larger artifacts, often recursively. The ResNet-152 architecture possesses hundreds of layers consisting of repeating patterns of groups of layers. Modules are represented by a class, and any subclass of it must define a forward propagation method that transforms its input into output and must store any necessary parameters. The section ends with an example of how to implement a custom module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78fdccb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T05:20:24.059162Z",
     "iopub.status.busy": "2023-02-10T05:20:24.058846Z",
     "iopub.status.idle": "2023-02-10T05:20:26.167207Z",
     "shell.execute_reply": "2023-02-10T05:20:26.165402Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638fe6c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T05:20:26.174190Z",
     "iopub.status.busy": "2023-02-10T05:20:26.173235Z",
     "iopub.status.idle": "2023-02-10T05:20:26.202850Z",
     "shell.execute_reply": "2023-02-10T05:20:26.199540Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sangyun/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7c2951c",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## [**A Custom Module**]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d629fc18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T05:20:26.213364Z",
     "iopub.status.busy": "2023-02-10T05:20:26.209527Z",
     "iopub.status.idle": "2023-02-10T05:20:26.222832Z",
     "shell.execute_reply": "2023-02-10T05:20:26.221808Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class nn.Module to perform\n",
    "        # the necessary initialization\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.out = nn.LazyLinear(10)\n",
    "\n",
    "    # Define the forward propagation of the model, that is, how to return the\n",
    "    # required model output based on the input X\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e128ee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T05:20:26.226773Z",
     "iopub.status.busy": "2023-02-10T05:20:26.226282Z",
     "iopub.status.idle": "2023-02-10T05:20:26.234142Z",
     "shell.execute_reply": "2023-02-10T05:20:26.233290Z"
    },
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22b81481",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## [**The Sequential Module**]\n",
    ":label:`subsec_model-construction-sequential`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeb59127",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T05:20:26.239483Z",
     "iopub.status.busy": "2023-02-10T05:20:26.237728Z",
     "iopub.status.idle": "2023-02-10T05:20:26.244834Z",
     "shell.execute_reply": "2023-02-10T05:20:26.244033Z"
    },
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.children():\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac3aa43c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T05:20:26.250629Z",
     "iopub.status.busy": "2023-02-10T05:20:26.249027Z",
     "iopub.status.idle": "2023-02-10T05:20:26.262799Z",
     "shell.execute_reply": "2023-02-10T05:20:26.261660Z"
    },
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net(X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c58cc399",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "\n",
    "\n",
    "## [**Executing Code in the Forward Propagation Method**]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d01ce60e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T05:20:26.269105Z",
     "iopub.status.busy": "2023-02-10T05:20:26.268773Z",
     "iopub.status.idle": "2023-02-10T05:20:26.281082Z",
     "shell.execute_reply": "2023-02-10T05:20:26.277684Z"
    },
    "origin_pos": 36,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20))\n",
    "        self.linear = nn.LazyLinear(20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2fecc0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T05:20:26.294617Z",
     "iopub.status.busy": "2023-02-10T05:20:26.287081Z",
     "iopub.status.idle": "2023-02-10T05:20:26.311494Z",
     "shell.execute_reply": "2023-02-10T05:20:26.309284Z"
    },
    "origin_pos": 40,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.5998, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42c25542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T05:20:26.318191Z",
     "iopub.status.busy": "2023-02-10T05:20:26.317746Z",
     "iopub.status.idle": "2023-02-10T05:20:26.356489Z",
     "shell.execute_reply": "2023-02-10T05:20:26.355665Z"
    },
    "origin_pos": 44,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0994, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
    "                                 nn.LazyLinear(32), nn.ReLU())\n",
    "        self.linear = nn.LazyLinear(16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
