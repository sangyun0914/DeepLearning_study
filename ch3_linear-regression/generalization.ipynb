{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba5c438d",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Generalization\n",
    ":label:`sec_generalization_basics`\n",
    "\n",
    "## Section Summary\n",
    "This section discusses the problem of generalization in machine learning and how to ensure that models discover general patterns rather than simply memorizing data. The risk of overfitting, where a model fits too closely to the training data, is also introduced, and regularization techniques to combat overfitting are mentioned. The section concludes with some rules of thumb, such as the use of validation sets for model selection and the importance of having enough data.\n",
    "Few rules of thumb:\n",
    "1. Use validation sets (or $K$*-fold cross-validation*) for model selection;\n",
    "1. More complex models often require more data;\n",
    "1. Relevant notions of complexity include both the number of parameters and the range of values that they are allowed to take;\n",
    "1. Keeping all else equal, more data almost always leads to better generalization;\n",
    "1. This entire talk of generalization is all predicated on the IID assumption. If we relax this assumption, allowing for distributions to shift between the train and testing periods, then we cannot say anything about generalization absent a further (perhaps milder) assumption.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Training Error and Generalization Error Summary\n",
    "The text discusses the concepts of training error and generalization error in the context of supervised learning. It explains that the training error is a statistic calculated on the training dataset, while the generalization error is an expectation taken with respect to the underlying distribution. The generalization error is estimated by applying the model to an independent test set. The text notes that the training error will in general be a biased estimate of the true error on the underlying population, and the central question is when we can expect the training error to be close to the population error.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Model Complexity Summary\n",
    "\n",
    "The text discusses the relationship between training and generalization errors in machine learning models. In simple models with abundant data, the two errors tend to be close, but with more complex models and/or fewer examples, the generalization gap grows. The text emphasizes that low training error does not necessarily mean low generalization error, especially in models capable of fitting arbitrary labels. One notion of complexity is the range of values that the parameters can take, and comparing complexity between different model classes can be difficult. In these cases, relying on holdout data, or validation set, is necessary to certify generalization. The text also introduces the idea of model complexity inspired by Karl Popper's criterion of falsifiability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Underfitting or Overfitting? Sumamry\n",
    "\n",
    "The comparison between training and validation errors is important, and there are two common situations to watch out for. The first situation is when the training and validation errors are both substantial with a small gap between them, which may indicate that the model is too simple and that a more complex model could capture the pattern better. This is known as underfitting. The second situation is when the training error is significantly lower than the validation error, indicating overfitting. However, overfitting is not always bad, and we usually care about reducing the generalization error regardless of the gap. If the training error is zero, then the generalization gap is equal to the generalization error, and reducing the gap is the only way to make progress.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Polynomial Curve Fitting Summary\n",
    ":label:`subsec_polynomial-curve-fitting`\n",
    "\n",
    "The text discusses the problem of polynomial curve fitting, where we try to find the polynomial of degree d to estimate the label y given training data consisting of a single feature x and a corresponding real-valued label y. The text explains that a higher-order polynomial function is more complex than a lower-order polynomial function, and always achieves lower (at worst, equal) training error relative to lower degree polynomials. The text also visualizes the relationship between polynomial degree (model complexity) and underfitting vs. overfitting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Dataset Size Summary\n",
    "\n",
    "The text discusses the importance of dataset size in avoiding overfitting. It states that with fewer samples in the training dataset, the generalization error is more likely to increase. Increasing the amount of training data typically results in a decrease in generalization error, and in general, more data never hurts. The text also suggests that for a fixed task and data distribution, the model's complexity should not increase more rapidly than the amount of data. Finally, it notes that deep learning typically outperforms linear models when many thousands of training examples are available.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Model Selection Summary\n",
    ":label:`subsec_generalization-model-selection`\n",
    "\n",
    "The text discusses the process of model selection and the importance of having separate training, validation, and test datasets. The test set should not be used for model selection as it can lead to overfitting, and we cannot estimate the generalization error on the same data used to train the model. In practice, a validation set is commonly used in addition to the training and test sets, but this creates ambiguity between the validation and test data. The accuracy reported in the book's experiments is the validation accuracy and not a true test set accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Cross-Validation Summary\n",
    "\n",
    "The text explains that when there is not enough training data to create a proper validation set, we can use $K$-fold cross-validation, where the training data is divided into $K$ non-overlapping subsets. The model is then trained $K$ times, each time using a different subset for validation and the remaining $K-1$ subsets for training. The training and validation errors are then averaged over the $K$ experiments to estimate the errors.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
